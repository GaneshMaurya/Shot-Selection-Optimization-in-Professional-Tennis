{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install gdown","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T19:56:25.091608Z","iopub.execute_input":"2025-05-06T19:56:25.092321Z","iopub.status.idle":"2025-05-06T19:56:29.580310Z","shell.execute_reply.started":"2025-05-06T19:56:25.092293Z","shell.execute_reply":"2025-05-06T19:56:29.579386Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.18.0)\nRequirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.6)\nRequirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.13.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.1.31)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import classification_report\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nimport logging\nfrom datetime import datetime\nimport os\nimport csv\nimport gdown\n\n# Set up logging\nlog_dir = '/kaggle/working/logs'\nos.makedirs(log_dir, exist_ok=True)\nlog_file = os.path.join(log_dir, f'training_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log')\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler(log_file),\n        logging.StreamHandler()\n    ]\n)\nlogger = logging.getLogger(__name__)\n\n# Configuration for hyperparameters (modify these for tuning)\nCONFIG = {\n    'random_seed': 42,\n    'test_size': 0.2,\n    'validation_split': 0.2,\n    'batch_size': 32,\n    'epochs': 100,\n    'early_stopping_patience': 10,\n    'learning_rate': 0.001,\n    'hidden_layers': [64, 32],  # Number of neurons in each hidden layer\n    'dropout_rate': 0.3,\n    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n    'log_metrics_file': '/kaggle/working/metrics.csv',\n    'gdrive_file_id': '16IH03soaKK15gvOO4t84ohCP-n2abCYV',  # Replace with your Google Drive file ID\n    'csv_file_name': 'dataset_subset.csv',  # Name of the CSV file after download\n}\n\n# Set random seed for reproducibility\ntorch.manual_seed(CONFIG['random_seed'])\nnp.random.seed(CONFIG['random_seed'])\nif CONFIG['device'] == 'cuda':\n    torch.cuda.manual_seed(CONFIG['random_seed'])\n\nclass TennisOutcomeNN(nn.Module):\n    \"\"\"Feedforward neural network for tennis outcome prediction.\"\"\"\n    def __init__(self, input_dim, num_classes, hidden_layers, dropout_rate):\n        super(TennisOutcomeNN, self).__init__()\n        layers = []\n        prev_dim = input_dim\n        \n        # Hidden layers\n        for units in hidden_layers:\n            layers.append(nn.Linear(prev_dim, units))\n            layers.append(nn.ReLU())\n            layers.append(nn.Dropout(dropout_rate))\n            prev_dim = units\n        \n        # Output layer\n        layers.append(nn.Linear(prev_dim, num_classes))\n        self.network = nn.Sequential(*layers)\n    \n    def forward(self, x):\n        return self.network(x)\n\ndef load_and_preprocess_data():\n    \"\"\"Download CSV from Google Drive and preprocess the dataset.\"\"\"\n    logger.info(\"Downloading CSV from Google Drive...\")\n    \n    # Download file using gdown\n    file_id = CONFIG['gdrive_file_id']\n    output_path = os.path.join('/kaggle/working', CONFIG['csv_file_name'])\n    gdown.download(f'https://drive.google.com/uc?id={file_id}', output_path, quiet=False)\n    \n    if not os.path.exists(output_path):\n        logger.error(f\"Failed to download file to {output_path}\")\n        raise FileNotFoundError(f\"File {output_path} not found after download\")\n    \n    logger.info(\"Loading dataset...\")\n    df = pd.read_csv(output_path)\n    \n    # Select features and target\n    numerical_features = ['rally_length', 'serve_depth']\n    categorical_features = [\n        'serve_type', 'serve_direction', 'shot_1_type', 'shot_1_direction',\n        'shot_2_type', 'shot_2_direction', 'last_shot_type', 'last_shot_direction'\n    ]\n    target = 'outcome'\n    \n    # Handle missing values\n    df[numerical_features] = df[numerical_features].fillna(0)\n    df[categorical_features] = df[categorical_features].fillna('unknown')\n    \n    # Convert categorical columns to strings to avoid type mismatch\n    logger.info(\"Converting categorical columns to strings...\")\n    for col in categorical_features:\n        # Log data types and unique values for debugging\n        logger.info(f\"Column {col} data types: {df[col].apply(type).value_counts().to_dict()}\")\n        logger.info(f\"Column {col} unique values: {df[col].unique()}\")\n        df[col] = df[col].astype(str)\n    \n    # Verify data types after conversion\n    logger.info(\"Data types after conversion:\")\n    for col in categorical_features:\n        logger.info(f\"Column {col} data type: {df[col].dtype}\")\n    \n    # Encode target variable\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(df[target])\n    \n    # Log class distribution\n    class_counts = pd.Series(y).value_counts()\n    class_names = label_encoder.inverse_transform(class_counts.index)\n    logger.info(\"Class distribution:\")\n    for name, count in zip(class_names, class_counts):\n        logger.info(f\"{name}: {count}\")\n    \n    # Features\n    X = df[numerical_features + categorical_features]\n    \n    # Preprocessing pipeline\n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('num', StandardScaler(), numerical_features),\n            ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n        ])\n    \n    # Split data\n    X_train, X_temp, y_train, y_temp = train_test_split(\n        X, y, test_size=CONFIG['test_size'] + CONFIG['validation_split'],\n        random_state=CONFIG['random_seed'], stratify=y\n    )\n    X_val, X_test, y_val, y_test = train_test_split(\n        X_temp, y_temp, test_size=CONFIG['test_size'] / (CONFIG['test_size'] + CONFIG['validation_split']),\n        random_state=CONFIG['random_seed'], stratify=y_temp\n    )\n    \n    # Fit and transform\n    X_train = preprocessor.fit_transform(X_train)\n    X_val = preprocessor.transform(X_val)\n    X_test = preprocessor.transform(X_test)\n    \n    # Convert to PyTorch tensors\n    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n    y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n    y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n    \n    # Create DataLoaders\n    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n    \n    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n    \n    logger.info(f\"Training data shape: {X_train.shape}\")\n    logger.info(f\"Validation data shape: {X_val.shape}\")\n    logger.info(f\"Test data shape: {X_test.shape}\")\n    \n    return train_loader, val_loader, test_loader, label_encoder, preprocessor\n\ndef train_model(model, train_loader, val_loader, criterion, optimizer, device):\n    \"\"\"Train the model with early stopping.\"\"\"\n    best_val_loss = float('inf')\n    patience_counter = 0\n    metrics = []\n    \n    # Initialize metrics CSV\n    with open(CONFIG['log_metrics_file'], 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Epoch', 'Train Loss', 'Train Accuracy', 'Val Loss', 'Val Accuracy'])\n    \n    logger.info(\"Starting model training...\")\n    for epoch in range(CONFIG['epochs']):\n        model.train()\n        train_loss = 0.0\n        train_correct = 0\n        train_total = 0\n        \n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item() * inputs.size(0)\n            _, predicted = torch.max(outputs, 1)\n            train_total += labels.size(0)\n            train_correct += (predicted == labels).sum().item()\n        \n        train_loss /= train_total\n        train_accuracy = train_correct / train_total\n        \n        # Validation\n        model.eval()\n        val_loss = 0.0\n        val_correct = 0\n        val_total = 0\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                \n                val_loss += loss.item() * inputs.size(0)\n                _, predicted = torch.max(outputs, 1)\n                val_total += labels.size(0)\n                val_correct += (predicted == labels).sum().item()\n        \n        val_loss /= val_total\n        val_accuracy = val_correct / val_total\n        \n        # Log metrics\n        metrics.append([epoch + 1, train_loss, train_accuracy, val_loss, val_accuracy])\n        with open(CONFIG['log_metrics_file'], 'a', newline='') as f:\n            writer = csv.writer(f)\n            writer.writerow([epoch + 1, train_loss, train_accuracy, val_loss, val_accuracy])\n        \n        logger.info(\n            f\"Epoch {epoch+1}/{CONFIG['epochs']} - \"\n            f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, \"\n            f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\"\n        )\n        \n        # Early stopping\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            patience_counter = 0\n            torch.save(model.state_dict(), '/kaggle/working/best_model.pth')\n        else:\n            patience_counter += 1\n            if patience_counter >= CONFIG['early_stopping_patience']:\n                logger.info(f\"Early stopping triggered after epoch {epoch+1}\")\n                break\n    \n    logger.info(\"Training completed.\")\n    return metrics\n\ndef evaluate_model(model, test_loader, label_encoder, device):\n    \"\"\"Evaluate the model on the test set.\"\"\"\n    model.eval()\n    y_true = []\n    y_pred = []\n    \n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs, 1)\n            y_true.extend(labels.cpu().numpy())\n            y_pred.extend(predicted.cpu().numpy())\n    \n    # Convert numerical labels back to class names\n    y_true = label_encoder.inverse_transform(y_true)\n    y_pred = label_encoder.inverse_transform(y_pred)\n    \n    # Classification report\n    report = classification_report(y_true, y_pred)\n    logger.info(f\"Classification Report:\\n{report}\")\n\ndef main():\n    \"\"\"Main function to run the prediction pipeline.\"\"\"\n    # Load and preprocess data\n    train_loader, val_loader, test_loader, label_encoder, preprocessor = load_and_preprocess_data()\n    \n    # Initialize model\n    input_dim = next(iter(train_loader))[0].shape[1]\n    num_classes = len(label_encoder.classes_)\n    model = TennisOutcomeNN(\n        input_dim=input_dim,\n        num_classes=num_classes,\n        hidden_layers=CONFIG['hidden_layers'],\n        dropout_rate=CONFIG['dropout_rate']\n    ).to(CONFIG['device'])\n    \n    # Loss and optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=CONFIG['learning_rate'])\n    \n    # Train model\n    metrics = train_model(model, train_loader, val_loader, criterion, optimizer, CONFIG['device'])\n    \n    # Load best model\n    model.load_state_dict(torch.load('/kaggle/working/best_model.pth'))\n    \n    # Evaluate model\n    evaluate_model(model, test_loader, label_encoder, CONFIG['device'])\n    \n    # Save model\n    torch.save(model.state_dict(), '/kaggle/working/tennis_outcome_model.pth')\n    logger.info(\"Model saved to '/kaggle/working/tennis_outcome_model.pth'\")\n    \n    # Instructions for tuning and Google Drive setup\n    logger.info(\"\"\"\n    To use a CSV file from Google Drive:\n    1. Right-click the CSV file in Google Drive, select 'Share', and set to 'Anyone with the link' or 'Viewer'.\n    2. Copy the shareable link (e.g., https://drive.google.com/file/d/FILE_ID/view?usp=sharing).\n    3. Extract the FILE_ID (string between '/d/' and '/view').\n    4. Update CONFIG['gdrive_file_id'] with the FILE_ID in this script.\n    \n    To tune hyperparameters, modify the CONFIG dictionary. Options include:\n    - learning_rate: Try [0.0001, 0.001, 0.01]\n    - hidden_layers: Try different architectures, e.g., [128, 64], [32, 16]\n    - dropout_rate: Try [0.2, 0.4, 0.5]\n    - batch_size: Try [16, 64, 128]\n    - epochs: Increase if model underfits\n    Metrics are saved to 'metrics.csv' for analysis.\n    Logs are saved to the 'logs' directory.\n    If the TypeError persists, check the logged unique values for categorical columns\n    and ensure the dataset has consistent data types.\n    \"\"\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-06T21:01:46.434518Z","iopub.execute_input":"2025-05-06T21:01:46.434946Z","iopub.status.idle":"2025-05-06T21:29:02.819945Z","shell.execute_reply.started":"2025-05-06T21:01:46.434918Z","shell.execute_reply":"2025-05-06T21:29:02.819045Z"}},"outputs":[{"name":"stderr","text":"Downloading...\nFrom (original): https://drive.google.com/uc?id=16IH03soaKK15gvOO4t84ohCP-n2abCYV\nFrom (redirected): https://drive.google.com/uc?id=16IH03soaKK15gvOO4t84ohCP-n2abCYV&confirm=t&uuid=f6cbd7f7-3e89-4f84-ad96-f5ef58574448\nTo: /kaggle/working/dataset_subset.csv\n100%|██████████| 143M/143M [00:03<00:00, 47.0MB/s] \n/tmp/ipykernel_31/1088405632.py:89: DtypeWarning: Columns (9,14) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(output_path)\n/tmp/ipykernel_31/1088405632.py:299: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('/kaggle/working/best_model.pth'))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, accuracy_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport os\nfrom datetime import datetime\nimport pickle\nimport logging\nimport warnings\nimport gdown\nwarnings.filterwarnings('ignore')\n\n# Setup logging\nlogging.basicConfig(filename='training_log_transformer.txt', level=logging.INFO, \n                    format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger()\n\ndef log_and_print(message):\n    logger.info(message)\n    print(message)\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nlog_and_print(f\"Using device: {device}\")\n\n# Load and preprocess data\ndef load_data(file_path):\n    log_and_print(\"Loading dataset...\")\n    df = pd.read_csv(file_path)\n    log_and_print(f\"Dataset loaded with {len(df)} rows and {len(df.columns)} columns\")\n    return df\n\ndef preprocess_data(df):\n    # Standardize outcome labels\n    outcome_mapping = {\n        'Winner': 'Winner', 'Forced Error': 'Forced Error', 'Unforced Error': 'Unforced Error',\n        'Ace': 'Winner', 'Double Fault': 'Unforced Error'\n    }\n    df['outcome'] = df['outcome'].map(outcome_mapping)\n    df = df.dropna(subset=['outcome'])\n    log_and_print(f\"After cleaning, dataset has {len(df)} rows\")\n\n    # Select features\n    feature_columns = [\n        'serve_type', 'serve_direction', 'serve_depth', 'is_second_serve',\n        'rally_length', 'shot_1_type', 'shot_1_direction', 'shot_1_depth',\n        'shot_2_type', 'shot_2_direction', 'shot_2_depth',\n        'shot_3_type', 'shot_3_direction', 'shot_3_depth',\n        'shot_4_type', 'shot_4_direction', 'shot_4_depth',\n        'shot_5_type', 'shot_5_direction', 'shot_5_depth',\n        'last_shot_type', 'last_shot_direction', 'last_shot_depth'\n    ]\n    df_features = df[feature_columns].copy()\n    df_target = df['outcome']\n\n    # Handle missing values\n    for col in df_features.columns:\n        if df_features[col].dtype == 'object':\n            df_features[col] = df_features[col].fillna('None')\n        else:\n            df_features[col] = df_features[col].fillna(0)\n\n    # Convert categorical columns to strings to avoid type mismatch\n    categorical_columns = [col for col in df_features.columns if df_features[col].dtype == 'object']\n    log_and_print(\"Converting categorical columns to strings...\")\n    for col in categorical_columns:\n        # Log data types and unique values for debugging\n        log_and_print(f\"Column {col} data types: {df_features[col].apply(type).value_counts().to_dict()}\")\n        log_and_print(f\"Column {col} unique values: {df_features[col].unique()}\")\n        df_features[col] = df_features[col].astype(str)\n    \n    # Verify data types after conversion\n    log_and_print(\"Data types after conversion:\")\n    for col in categorical_columns:\n        log_and_print(f\"Column {col} data type: {df_features[col].dtype}\")\n\n    # Encode categorical features\n    vocab_sizes = {}\n    encoders = {}\n    for col in categorical_columns:\n        encoders[col] = LabelEncoder()\n        df_features[col] = encoders[col].fit_transform(df_features[col])\n        vocab_sizes[col] = len(encoders[col].classes_)\n\n    # Encode target\n    target_encoder = LabelEncoder()\n    y = target_encoder.fit_transform(df_target)\n    log_and_print(f\"Class distribution: {dict(zip(target_encoder.classes_, np.bincount(y)))}\")\n\n    return df_features, y, categorical_columns, vocab_sizes, encoders, target_encoder\n\n# Custom Dataset\nclass TennisDataset(Dataset):\n    def __init__(self, X, y, categorical_columns):\n        self.X = X\n        self.y = y\n        self.categorical_columns = categorical_columns\n        self.numerical_columns = [col for col in X.columns if col not in categorical_columns]\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        # Prepare sequence: treat each feature as a token\n        cat_features = torch.tensor([self.X[col].iloc[idx] for col in self.categorical_columns], dtype=torch.long)\n        num_features = torch.tensor([self.X[col].iloc[idx] for col in self.numerical_columns], dtype=torch.float)\n        label = torch.tensor(self.y[idx], dtype=torch.long)\n        return cat_features, num_features, label\n\n# Transformer Model\nclass TennisTransformer(nn.Module):\n    def __init__(self, vocab_sizes, num_numerical, d_model=64, nhead=4, num_layers=2, dim_feedforward=128, num_classes=3, dropout=0.3):\n        super(TennisTransformer, self).__init__()\n        self.d_model = d_model\n\n        # Embedding layers for categorical features\n        self.embeddings = nn.ModuleDict({\n            col: nn.Embedding(vocab_size, d_model) for col, vocab_size in vocab_sizes.items()\n        })\n\n        # Linear layer for numerical features\n        self.numerical_layer = nn.Linear(num_numerical, d_model)\n\n        # Transformer encoder\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True\n        )\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n\n        # Output layer\n        self.fc = nn.Linear(d_model, num_classes)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, cat_features, num_features):\n        # Embed categorical features\n        cat_embeds = []\n        for i, col in enumerate(self.embeddings.keys()):\n            emb = self.embeddings[col](cat_features[:, i])\n            cat_embeds.append(emb)\n        cat_embeds = torch.stack(cat_embeds, dim=1)\n\n        # Process numerical features\n        num_embeds = self.numerical_layer(num_features).unsqueeze(1)\n\n        # Combine embeddings\n        x = torch.cat([cat_embeds, num_embeds], dim=1)  # [batch, seq_len, d_model]\n        # log_and_print(f\"Input to transformer shape: {x.shape}\")\n\n        # Transformer encoding\n        x = self.transformer_encoder(x)\n        x = x.mean(dim=1)  # Global average pooling\n        x = self.dropout(x)\n        out = self.fc(x)\n        return out\n\n# Training function\ndef train_model(model, train_loader, val_loader, num_epochs=50, patience=5):\n    criterion = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 1.1, 1.0]).to(device))  # Slight weight for Forced Error\n    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n\n    best_val_loss = float('inf')\n    epochs_no_improve = 0\n    train_losses, val_losses = [], []\n\n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0\n        for cat_features, num_features, labels in train_loader:\n            cat_features, num_features, labels = cat_features.to(device), num_features.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(cat_features, num_features)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item() * len(labels)\n        train_loss /= len(train_loader.dataset)\n        train_losses.append(train_loss)\n\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for cat_features, num_features, labels in val_loader:\n                cat_features, num_features, labels = cat_features.to(device), num_features.to(device), labels.to(device)\n                outputs = model(cat_features, num_features)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item() * len(labels)\n        val_loss /= len(val_loader.dataset)\n        val_losses.append(val_loss)\n\n        scheduler.step(val_loss)\n        log_and_print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n\n        # Early stopping\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            epochs_no_improve = 0\n            torch.save(model.state_dict(), 'model_output_transformer/best_model.pth')\n        else:\n            epochs_no_improve += 1\n            if epochs_no_improve >= patience:\n                log_and_print(\"Early stopping triggered\")\n                break\n\n    return train_losses, val_losses\n\n# Evaluation function\ndef evaluate_model(model, loader, target_encoder):\n    model.eval()\n    y_true, y_pred = [], []\n    with torch.no_grad():\n        for cat_features, num_features, labels in loader:\n            cat_features, num_features = cat_features.to(device), num_features.to(device)\n            outputs = model(cat_features, num_features)\n            _, preds = torch.max(outputs, 1)\n            y_true.extend(labels.numpy())\n            y_pred.extend(preds.cpu().numpy())\n\n    # Classification report\n    report = classification_report(y_true, y_pred, target_names=target_encoder.classes_, output_dict=True)\n    log_and_print(\"\\nClassification Report:\")\n    log_and_print(classification_report(y_true, y_pred, target_names=target_encoder.classes_))\n\n    # Confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_encoder.classes_, yticklabels=target_encoder.classes_)\n    plt.title('Confusion Matrix')\n    plt.savefig('model_output_transformer/confusion_matrix.png')\n    plt.close()\n\n    # Precision-Recall curves\n    y_scores = []\n    model.eval()\n    with torch.no_grad():\n        for cat_features, num_features, _ in loader:\n            cat_features, num_features = cat_features.to(device), num_features.to(device)\n            outputs = model(cat_features, num_features)\n            y_scores.append(torch.softmax(outputs, dim=1).cpu().numpy())\n    y_scores = np.vstack(y_scores)\n    y_true_bin = np.eye(len(target_encoder.classes_))[y_true]\n\n    plt.figure(figsize=(10, 8))\n    for i, class_name in enumerate(target_encoder.classes_):\n        precision, recall, _ = precision_recall_curve(y_true_bin[:, i], y_scores[:, i])\n        plt.plot(recall, precision, label=f'{class_name} (AP={np.mean(precision):.2f})')\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Precision-Recall Curve')\n    plt.legend()\n    plt.savefig('model_output_transformer/precision_recall_curve.png')\n    plt.close()\n\n    return report, cm\n\n# Main execution\ndef main():\n    start_time = datetime.now()\n    output_dir = 'model_output_transformer'\n    os.makedirs(output_dir, exist_ok=True)\n\n    CONFIG = {\n        'log_metrics_file': '/kaggle/working/metrics.csv',\n        'gdrive_file_id': '16IH03soaKK15gvOO4t84ohCP-n2abCYV',  # Replace with your Google Drive file ID\n        'csv_file_name': 'dataset_subset.csv',  # Name of the CSV file after download\n    }\n\n    # Load and preprocess data\n    file_id = CONFIG['gdrive_file_id']\n    output_path = os.path.join('/kaggle/working', CONFIG['csv_file_name'])\n    gdown.download(f'https://drive.google.com/uc?id={file_id}', output_path, quiet=False)\n    df = load_data(output_path)\n    X, y, categorical_columns, vocab_sizes, encoders, target_encoder = preprocess_data(df)\n\n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n    log_and_print(f\"Training set: {len(X_train)} samples, Test set: {len(X_test)} samples\")\n\n    # Create datasets and dataloaders\n    train_dataset = TennisDataset(X_train, y_train, categorical_columns)\n    test_dataset = TennisDataset(X_test, y_test, categorical_columns)\n    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n\n    # Initialize model\n    num_numerical = len([col for col in X.columns if col not in categorical_columns])\n    model = TennisTransformer(vocab_sizes, num_numerical, d_model=64, nhead=4, num_layers=2, dim_feedforward=128, num_classes=3).to(device)\n    log_and_print(f\"Model initialized with {sum(p.numel() for p in model.parameters())} parameters\")\n\n    # Train model\n    log_and_print(\"Starting training...\")\n    train_losses, val_losses = train_model(model, train_loader, test_loader)\n\n    # Plot training history\n    plt.figure(figsize=(10, 5))\n    plt.plot(train_losses, label='Train Loss')\n    plt.plot(val_losses, label='Validation Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.savefig('model_output_transformer/training_history.png')\n    plt.close()\n\n    # Evaluate model\n    log_and_print(\"Evaluating model...\")\n    report, cm = evaluate_model(model, test_loader, target_encoder)\n\n    # Check for overfitting\n    train_acc = accuracy_score(y_train, [model(torch.tensor(X_train[cat_cols].values, dtype=torch.long).to(device), \n                                            torch.tensor(X_train[num_cols].values, dtype=torch.float).to(device)).argmax(1).cpu().numpy() \n                                       for cat_cols, num_cols, _ in [next(iter(train_loader))]][0])\n    test_acc = report['accuracy']\n    log_and_print(f\"Training Accuracy: {train_acc:.4f}, Test Accuracy: {test_acc:.4f}, Difference: {train_acc - test_acc:.4f}\")\n\n    # Save artifacts\n    with open(os.path.join(output_dir, 'classification_report.pkl'), 'wb') as f:\n        pickle.dump(report, f)\n    with open(os.path.join(output_dir, 'target_encoder.pkl'), 'wb') as f:\n        pickle.dump(target_encoder, f)\n    with open(os.path.join(output_dir, 'feature_encoders.pkl'), 'wb') as f:\n        pickle.dump(encoders, f)\n    with open(os.path.join(output_dir, 'feature_columns.pkl'), 'wb') as f:\n        pickle.dump(X.columns.tolist(), f)\n\n    # Prediction function\n    def predict(serve_data, rally_data):\n        model.eval()\n        with torch.no_grad():\n            # Preprocess input\n            input_df = pd.DataFrame([serve_data | rally_data])\n            for col in X.columns:\n                if col not in input_df:\n                    input_df[col] = 'None' if col in categorical_columns else 0\n            for col in categorical_columns:\n                input_df[col] = encoders[col].transform(input_df[col])\n            cat_features = torch.tensor([input_df[col].iloc[0] for col in categorical_columns], dtype=torch.long).unsqueeze(0).to(device)\n            num_features = torch.tensor([input_df[col].iloc[0] for col in X.columns if col not in categorical_columns], dtype=torch.float).unsqueeze(0).to(device)\n            output = model(cat_features, num_features)\n            pred = torch.argmax(output, dim=1).cpu().numpy()\n            return target_encoder.inverse_transform(pred)[0]\n\n    # Save prediction function\n    with open(os.path.join(output_dir, 'predict.pkl'), 'wb') as f:\n        pickle.dump(predict, f)\n\n    log_and_print(f\"Training completed in {datetime.now() - start_time}\")\n    log_and_print(\"Artifacts saved in model_output_transformer/\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T22:59:16.982665Z","iopub.execute_input":"2025-05-06T22:59:16.983008Z","iopub.status.idle":"2025-05-07T00:30:16.313987Z","shell.execute_reply.started":"2025-05-06T22:59:16.982985Z","shell.execute_reply":"2025-05-07T00:30:16.312797Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"Downloading...\nFrom (original): https://drive.google.com/uc?id=16IH03soaKK15gvOO4t84ohCP-n2abCYV\nFrom (redirected): https://drive.google.com/uc?id=16IH03soaKK15gvOO4t84ohCP-n2abCYV&confirm=t&uuid=2e74df7f-d4ad-45b8-9513-8b89027ff1a6\nTo: /kaggle/working/dataset_subset.csv\n100%|██████████| 143M/143M [00:01<00:00, 95.6MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Loading dataset...\nDataset loaded with 991359 rows and 41 columns\nAfter cleaning, dataset has 960585 rows\nConverting categorical columns to strings...\nColumn serve_type data types: {<class 'int'>: 540242, <class 'str'>: 420343}\nColumn serve_type unique values: [4 5 6 0 '4' '5' '6' '0' 'g']\nColumn shot_1_type data types: {<class 'str'>: 960585}\nColumn shot_1_type unique values: ['b' 'f' 'r' 's' 'None' 'm' 'u' 'l' 'q' 'y' 'i' 't' 'w' '3' 'h' 'v' 'o'\n '7' '2' '1' 'z' 'e' '&']\nColumn shot_2_type data types: {<class 'str'>: 960585}\nColumn shot_2_type unique values: ['b' 'f' 'None' 'z' 's' 'v' 'u' 'h' 'j' 'y' 't' 'r' 'o' 'i' 'm' 'k' 'l'\n 'p' 'q']\nColumn shot_3_type data types: {<class 'str'>: 960585}\nColumn shot_3_type unique values: ['b' 'None' 'f' 'm' 's' 'l' 'r' 'y' 'i' 'z' 'v' 'u' 'p' 'h' 'j' 'o' 't'\n 'q' 'k']\nColumn shot_4_type data types: {<class 'str'>: 960585}\nColumn shot_4_type unique values: ['None' 'b' 'i' 'f' 'o' 'v' 'z' 'r' 'h' 's' 'm' 'y' 'u' 't' 'l' 'j' 'k'\n 'q' 'p']\nColumn shot_5_type data types: {<class 'str'>: 960585}\nColumn shot_5_type unique values: ['None' 'b' 'i' 'f' 'm' 'z' 'v' 's' 'o' 'r' 'l' 'j' 'u' 'y' 't' 'h' 'q'\n 'k' 'p']\nColumn last_shot_type data types: {<class 'str'>: 960585}\nColumn last_shot_type unique values: ['b' 'f' 'r' 'i' 'o' 's' 'v' 'None' 'z' 'u' 'm' 'l' 'j' 'y' 't' 'k' 'h'\n 'p' 'q' '3' '&']\nData types after conversion:\nColumn serve_type data type: object\nColumn shot_1_type data type: object\nColumn shot_2_type data type: object\nColumn shot_3_type data type: object\nColumn shot_4_type data type: object\nColumn shot_5_type data type: object\nColumn last_shot_type data type: object\nClass distribution: {'Forced Error': 308713, 'Unforced Error': 338515, 'Winner': 313357}\nTraining set: 768468 samples, Test set: 192117 samples\nModel initialized with 76227 parameters\nStarting training...\nEpoch 1/50, Train Loss: 0.6617, Val Loss: 0.6193\nEpoch 2/50, Train Loss: 0.6223, Val Loss: 0.6077\nEpoch 3/50, Train Loss: 0.6136, Val Loss: 0.6020\nEpoch 4/50, Train Loss: 0.6092, Val Loss: 0.5989\nEpoch 5/50, Train Loss: 0.6060, Val Loss: 0.6013\nEpoch 6/50, Train Loss: 0.6043, Val Loss: 0.6009\nEpoch 7/50, Train Loss: 0.6019, Val Loss: 0.6035\nEpoch 8/50, Train Loss: 0.6003, Val Loss: 0.6026\nEpoch 9/50, Train Loss: 0.5950, Val Loss: 0.5954\nEpoch 10/50, Train Loss: 0.5933, Val Loss: 0.5906\nEpoch 11/50, Train Loss: 0.5923, Val Loss: 0.5955\nEpoch 12/50, Train Loss: 0.5919, Val Loss: 0.5903\nEpoch 13/50, Train Loss: 0.5914, Val Loss: 0.5900\nEpoch 14/50, Train Loss: 0.5909, Val Loss: 0.5907\nEpoch 15/50, Train Loss: 0.5902, Val Loss: 0.5905\nEpoch 16/50, Train Loss: 0.5897, Val Loss: 0.5904\nEpoch 17/50, Train Loss: 0.5892, Val Loss: 0.5904\nEpoch 18/50, Train Loss: 0.5864, Val Loss: 0.5869\nEpoch 19/50, Train Loss: 0.5861, Val Loss: 0.5888\nEpoch 20/50, Train Loss: 0.5854, Val Loss: 0.5881\nEpoch 21/50, Train Loss: 0.5855, Val Loss: 0.5877\nEpoch 22/50, Train Loss: 0.5853, Val Loss: 0.5861\nEpoch 23/50, Train Loss: 0.5851, Val Loss: 0.5847\nEpoch 24/50, Train Loss: 0.5851, Val Loss: 0.5859\nEpoch 25/50, Train Loss: 0.5846, Val Loss: 0.5868\nEpoch 26/50, Train Loss: 0.5845, Val Loss: 0.5847\nEpoch 27/50, Train Loss: 0.5844, Val Loss: 0.5893\nEpoch 28/50, Train Loss: 0.5829, Val Loss: 0.5849\nEpoch 29/50, Train Loss: 0.5824, Val Loss: 0.5857\nEpoch 30/50, Train Loss: 0.5823, Val Loss: 0.5854\nEpoch 31/50, Train Loss: 0.5822, Val Loss: 0.5849\nEarly stopping triggered\nEvaluating model...\n\nClassification Report:\n                precision    recall  f1-score   support\n\n  Forced Error       0.83      0.71      0.77     61743\nUnforced Error       0.65      0.79      0.71     67703\n        Winner       0.78      0.71      0.74     62671\n\n      accuracy                           0.74    192117\n     macro avg       0.75      0.74      0.74    192117\n  weighted avg       0.75      0.74      0.74    192117\n\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/2871424856.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_31/2871424856.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;31m# Check for overfitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     train_acc = accuracy_score(y_train, [model(torch.tensor(X_train[cat_cols].values, dtype=torch.long).to(device), \n\u001b[0m\u001b[1;32m    317\u001b[0m                                             torch.tensor(X_train[num_cols].values, dtype=torch.float).to(device)).argmax(1).cpu().numpy() \n\u001b[1;32m    318\u001b[0m                                        for cat_cols, num_cols, _ in [next(iter(train_loader))]][0])\n","\u001b[0;32m/tmp/ipykernel_31/2871424856.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;31m# Check for overfitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     train_acc = accuracy_score(y_train, [model(torch.tensor(X_train[cat_cols].values, dtype=torch.long).to(device), \n\u001b[0m\u001b[1;32m    317\u001b[0m                                             torch.tensor(X_train[num_cols].values, dtype=torch.float).to(device)).argmax(1).cpu().numpy() \n\u001b[1;32m    318\u001b[0m                                        for cat_cols, num_cols, _ in [next(iter(train_loader))]][0])\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4107\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4108\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4110\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6198\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6200\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6247\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnmissing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6248\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnmissing\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6249\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6251\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: \"None of [Index([   (tensor(1), tensor(6), tensor(0), tensor(0), tensor(0), tensor(0), tensor(3)),\\n          (tensor(3), tensor(5), tensor(0), tensor(0), tensor(0), tensor(0), tensor(2)),\\n         (tensor(2), tensor(6), tensor(2), tensor(1), tensor(13), tensor(2), tensor(4)),\\n          (tensor(3), tensor(8), tensor(0), tensor(0), tensor(0), tensor(0), tensor(4)),\\n       (tensor(1), tensor(15), tensor(1), tensor(13), tensor(18), tensor(1), tensor(3)),\\n         (tensor(2), tensor(8), tensor(12), tensor(2), tensor(1), tensor(2), tensor(4)),\\n         (tensor(3), tensor(8), tensor(2), tensor(13), tensor(2), tensor(2), tensor(4)),\\n       (tensor(1), tensor(16), tensor(18), tensor(7), tensor(9), tensor(0), tensor(11)),\\n          (tensor(3), tensor(5), tensor(0), tensor(0), tensor(0), tensor(0), tensor(2)),\\n        (tensor(3), tensor(6), tensor(2), tensor(13), tensor(2), tensor(2), tensor(18)),\\n       ...\\n          (tensor(2), tensor(8), tensor(0), tensor(0), tensor(0), tensor(0), tensor(4)),\\n         (tensor(1), tensor(8), tensor(2), tensor(2), tensor(12), tensor(1), tensor(4)),\\n          (tensor(1), tensor(6), tensor(0), tensor(0), tensor(0), tensor(0), tensor(3)),\\n         (tensor(2), tensor(8), tensor(1), tensor(1), tensor(1), tensor(2), tensor(18)),\\n         (tensor(2), tensor(6), tensor(1), tensor(13), tensor(2), tensor(0), tensor(4)),\\n          (tensor(1), tensor(8), tensor(2), tensor(8), tensor(5), tensor(0), tensor(7)),\\n          (tensor(2), tensor(8), tensor(2), tensor(1), tensor(2), tensor(2), tensor(3)),\\n         (tensor(3), tensor(6), tensor(12), tensor(1), tensor(0), tensor(0), tensor(3)),\\n         (tensor(1), tensor(16), tensor(2), tensor(0), tensor(0), tensor(0), tensor(4)),\\n         (tensor(3), tensor(16), tensor(2), tensor(0), tensor(0), tensor(0), tensor(4))],\\n      dtype='object', length=256)] are in the [columns]\""],"ename":"KeyError","evalue":"\"None of [Index([   (tensor(1), tensor(6), tensor(0), tensor(0), tensor(0), tensor(0), tensor(3)),\\n          (tensor(3), tensor(5), tensor(0), tensor(0), tensor(0), tensor(0), tensor(2)),\\n         (tensor(2), tensor(6), tensor(2), tensor(1), tensor(13), tensor(2), tensor(4)),\\n          (tensor(3), tensor(8), tensor(0), tensor(0), tensor(0), tensor(0), tensor(4)),\\n       (tensor(1), tensor(15), tensor(1), tensor(13), tensor(18), tensor(1), tensor(3)),\\n         (tensor(2), tensor(8), tensor(12), tensor(2), tensor(1), tensor(2), tensor(4)),\\n         (tensor(3), tensor(8), tensor(2), tensor(13), tensor(2), tensor(2), tensor(4)),\\n       (tensor(1), tensor(16), tensor(18), tensor(7), tensor(9), tensor(0), tensor(11)),\\n          (tensor(3), tensor(5), tensor(0), tensor(0), tensor(0), tensor(0), tensor(2)),\\n        (tensor(3), tensor(6), tensor(2), tensor(13), tensor(2), tensor(2), tensor(18)),\\n       ...\\n          (tensor(2), tensor(8), tensor(0), tensor(0), tensor(0), tensor(0), tensor(4)),\\n         (tensor(1), tensor(8), tensor(2), tensor(2), tensor(12), tensor(1), tensor(4)),\\n          (tensor(1), tensor(6), tensor(0), tensor(0), tensor(0), tensor(0), tensor(3)),\\n         (tensor(2), tensor(8), tensor(1), tensor(1), tensor(1), tensor(2), tensor(18)),\\n         (tensor(2), tensor(6), tensor(1), tensor(13), tensor(2), tensor(0), tensor(4)),\\n          (tensor(1), tensor(8), tensor(2), tensor(8), tensor(5), tensor(0), tensor(7)),\\n          (tensor(2), tensor(8), tensor(2), tensor(1), tensor(2), tensor(2), tensor(3)),\\n         (tensor(3), tensor(6), tensor(12), tensor(1), tensor(0), tensor(0), tensor(3)),\\n         (tensor(1), tensor(16), tensor(2), tensor(0), tensor(0), tensor(0), tensor(4)),\\n         (tensor(3), tensor(16), tensor(2), tensor(0), tensor(0), tensor(0), tensor(4))],\\n      dtype='object', length=256)] are in the [columns]\"","output_type":"error"}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}